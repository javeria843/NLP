{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a125ed0e-6356-44b3-91b2-f2d0d086fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52eb6e60-42fa-44da-b294-9176a8b3cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58f0515-2979-4996-8b76-e8d42ce8f7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: c:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpxqe13w8b\n",
      "Requirement already satisfied: pip in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m ensurepip --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad1fd6b-6d13-4c07-944c-01029bacf577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae5d16-17ce-4828-b43a-50aabf334412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b5683-7cb2-4a1f-bba6-3071ac819cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2ba5c5-8abf-4696-a450-931d4fda694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use', 'logistic', 'regression', ',', 'and', 'naïve', 'Bayes', ',', 'and', 'word', 'vectors', 'to', 'implement', 'sentiment', 'analysis', ',', 'complete', 'analogies', '&', 'translate', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "##### ENGLISH TEXT TOKENS\n",
    "\n",
    "text = \"Use logistic regression, and naïve Bayes, and word vectors to implement sentiment analysis, complete analogies & translate words.\"\n",
    "tokens = word_tokenize(text)\n",
    "# print(tokens.count(\"and\"))\n",
    "print(tokens)\n",
    "\n",
    "##### URDU TEXT TOKENS\n",
    "# urdu_text = \"جذباتی تجزیے، مکمل تشبیہات اور الفاظ کا ترجمہ کرنے کے لیے لاجسٹک ریگریشن، ناواقف بیز، اور لفظ ویکٹر کا استعمال کریں۔\"\n",
    "# tokens = word_tokenize(urdu_text)\n",
    "# print(tokens)\n",
    "\n",
    "##### CHINESE TEXT TOKENS\n",
    "\n",
    "# chinese_text = \"使用邏輯迴歸、樸素貝葉斯和詞向量來實現情緒分析、完成類比和翻譯單字。\"\n",
    "# tokens = word_tokenize(chinese_text)\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0082433-f947-4056-bc0f-ccf5e72d20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a cat, if you want it please take her.', 'Also please take care of her.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is a cat, if you want it please take her. Also please take care of her.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c398334e-1279-45de-9bd3-46efb2b4f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Success', 'final', ',', 'failure', 'fatal', ':', 'courage', 'continue', 'counts', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"stopwords\")\n",
    "words= word_tokenize(\"Success is not final, failure is not fatal: It is the courage to continue that counts.\")\n",
    "filtered= [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df759906-7ad1-4a18-81d0-4ee69e842881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "\t\t\t\tshowing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7088473-005a-4d6b-abc8-0025a546069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dda7238-efcf-49c1-afed-d8f540d4c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8f1168-7f6b-46f1-bffa-8c6d91f01212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "words= [\"python\", \"pythoning\" , \"pythoner\", \"pythonly\"]\n",
    "words2=[\"class\",\"classing\",\"classification\"]\n",
    "stemmed=[ps.stem(w) for w in words2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f8c354-2eef-49a7-861f-8388bb272612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'class', 'classif']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4554a42-39e2-482a-bc84-a7c2ac40f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab15eb0-4683-438d-a067-242cc08d4b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3869b9-74e7-4c57-a576-eb788db7dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edb6079-70d6-4d8f-9a1c-90dacca4d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1f1eb8-305f-48c6-bc9e-ceb492edfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(\"she is learning nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9dfaf1-c07f-4a75-a478-b146ab836ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2=word_tokenize(\"The provided Python code demonstrates stopword removal using the Natural Language Toolkit (NLTK) library. In the first step, the sample sentence, which reads “This is a sample sentence, showing off the stop words filtration,” is tokenized into words using the word_tokenize function. The code then filters out stopwords by converting each word to lowercase and checking its presence in the set of English stopwords obtained from NLTK. The resulting filtered_sentence is printed, showcasing both lowercased and original versions, providing a cleaned version of the sentence with common English stopwords removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d67d4cc-ef8d-46fd-8b00-4e7ea399e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('provided', 'JJ'), ('Python', 'NNP'), ('code', 'NN'), ('demonstrates', 'VBZ'), ('stopword', 'JJ'), ('removal', 'NN'), ('using', 'VBG'), ('the', 'DT'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Toolkit', 'NNP'), ('(', '('), ('NLTK', 'NNP'), (')', ')'), ('library', 'NN'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), (',', ','), ('the', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), (',', ','), ('which', 'WDT'), ('reads', 'VBZ'), ('“', 'NN'), ('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), (',', ','), ('showing', 'VBG'), ('off', 'RP'), ('the', 'DT'), ('stop', 'NN'), ('words', 'NNS'), ('filtration', 'NN'), (',', ','), ('”', 'NNP'), ('is', 'VBZ'), ('tokenized', 'VBN'), ('into', 'IN'), ('words', 'NNS'), ('using', 'VBG'), ('the', 'DT'), ('word_tokenize', 'NN'), ('function', 'NN'), ('.', '.'), ('The', 'DT'), ('code', 'NN'), ('then', 'RB'), ('filters', 'VBZ'), ('out', 'RP'), ('stopwords', 'NNS'), ('by', 'IN'), ('converting', 'VBG'), ('each', 'DT'), ('word', 'NN'), ('to', 'TO'), ('lowercase', 'VB'), ('and', 'CC'), ('checking', 'VBG'), ('its', 'PRP$'), ('presence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('English', 'NNP'), ('stopwords', 'NNS'), ('obtained', 'VBN'), ('from', 'IN'), ('NLTK', 'NNP'), ('.', '.'), ('The', 'DT'), ('resulting', 'VBG'), ('filtered_sentence', 'NN'), ('is', 'VBZ'), ('printed', 'VBN'), (',', ','), ('showcasing', 'VBG'), ('both', 'DT'), ('lowercased', 'VBN'), ('and', 'CC'), ('original', 'JJ'), ('versions', 'NNS'), (',', ','), ('providing', 'VBG'), ('a', 'DT'), ('cleaned', 'JJ'), ('version', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), ('with', 'IN'), ('common', 'JJ'), ('English', 'NNP'), ('stopwords', 'NNS'), ('removed', 'VBD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcc59b-2e07-48f0-a417-b4f03a3e7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7d12f-f5a6-46f8-8b53-c0e9addd042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b3ea66-713d-446d-9f1b-38902b15c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec556a-5509-487a-9a6c-3c1d939db257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e21938-0716-43d9-ae4f-8ea3dc808b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"this is nlp text\")\n",
    "doc_token=[token.text for token in doc]\n",
    "print(doc_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad3b267-141f-4f7c-bf33-cb3393848f0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc=\u001b[43mnlp\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mthis is  a text\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m span=doc[\u001b[32m2\u001b[39m:\u001b[32m4\u001b[39m]\n\u001b[32m      3\u001b[39m span.text\n",
      "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"this is  a text\")\n",
    "span=doc[2:4]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ab8dbe-b1cc-4d60-91bd-07b30a0a0dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      2\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a7860a-b1f7-4f61-8617-f8dc7d6884c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'news_story.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnews_story.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     news_text = f.read()\n\u001b[32m      4\u001b[39m news_text[:\u001b[32m500\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'news_story.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"news_story.txt\",\"r\") as f:\n",
    "    news_text = f.read()\n",
    "    \n",
    "news_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "685e0b18-e723-4af5-ba82-fa62e449b65b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc=\u001b[43mnlp\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mThis apple is sweet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m doc_entities=[(ent.text, ent.label) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ent]\n",
      "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"This apple is sweet\")\n",
    "doc_entities=[(ent.text, ent.label) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "577b2dcc-1ca2-43a4-bd2d-01e7f12bd3b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdoc_entities\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'doc_entities' is not defined"
     ]
    }
   ],
   "source": [
    "print(doc_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcdc6c45-ba02-424b-97cc-84f1af2b0dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc =\u001b[43mnlp\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mpaistan is peacful country\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m doc_entities= [(doc.text,doc.label)\u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents]\n",
      "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc =nlp(\"paistan is peacful country\")\n",
    "doc_entities= [(doc.text,doc.label)for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df014aa0-8d20-4f0b-9116-015abeb0e431",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspacy\u001b[49m.explain(\u001b[33m\"\u001b[39m\u001b[33mGPE\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9edba2cc-ad78-4f7d-8d4a-f4c80a03b9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d6f773-bcec-42b5-b9c0-ed99efff640a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'displacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdisplacy\u001b[49m.render(doc,style=\u001b[33m\"\u001b[39m\u001b[33ment\u001b[39m\u001b[33m\"\u001b[39m, jupyter=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'displacy' is not defined"
     ]
    }
   ],
   "source": [
    "displacy.render(doc,style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8413f515-f387-40cf-aae2-c2eb7edc926e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (918188160.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m spacy download en_core_web_md\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d5bfeb-d135-44ca-ba1b-dc38d11ece12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      2\u001b[39m nlp=spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_md\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb6b0516-2153-4610-8833-280aa8d59f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1= (\"I love artificial intelligence.\")\n",
    "set2=(\"I like machine learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dff3b9b-1ef9-4c1a-8d0a-f4a877804ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m similarity = \u001b[43mset1\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity\u001b[49m(set2)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'similarity'"
     ]
    }
   ],
   "source": [
    "similarity = set1.similarity(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58f600a8-0428-43f5-8dc1-9f39cdeb6a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSimilarity score:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msimilarity\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'similarity' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Similarity score:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858b446a-fbea-4fc1-9d55-08632720f920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Similar sentences\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m a = \u001b[43mnlp\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mHe is playing football.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m b = nlp(\u001b[33m\"\u001b[39m\u001b[33mHe plays soccer.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFootball vs Soccer:\u001b[39m\u001b[33m\"\u001b[39m, a.similarity(b))  \u001b[38;5;66;03m# High\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Similar sentences\n",
    "a = nlp(\"He is playing football.\")\n",
    "b = nlp(\"He plays soccer.\")\n",
    "\n",
    "print(\"Football vs Soccer:\", a.similarity(b))  # High\n",
    "\n",
    "# Dissimilar sentences\n",
    "x = nlp(\"She likes pizza.\")\n",
    "y = nlp(\"The sun is very hot.\")\n",
    "\n",
    "print(\"Pizza vs Sun:\", x.similarity(y))  # Low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e15f81-cff1-485f-8546-e0e84888bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Make sure you downloaded it\n",
    "\n",
    "# Strings ko Doc objects me convert karo\n",
    "sentence1 = nlp(\"I love AI\")\n",
    "sentence2 = nlp(\"I like ML\")\n",
    "\n",
    "# Ab similarity kaam karega\n",
    "print(\"Similarity Score:\", sentence1.similarity(sentence2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
