bluberry enthusiastai httpshuggingfacecodatasetsdairaiemotion im gonna give distilled bird fine tuned emotion model lets go ahead create training argument thereafter pass trainer training done write training args thats training argument equal training arguments pass first output dir output dir directory model stored model name whatever give mean stored current directory stored running model training model number training box going give thereafter learning rate give power minus thats learning rate passing current model thereafter pass per device train batch size okay dont think need pass anyway check per device evaluation size default batch size think change well write per device size equal besides assigned similarly say per device train bash size equal size something like thereafter given pass weight underscore decay equal thats mean thats training happen would little weight dk done thereafter simply write evaluation strategy im going write apoc thereafter okay thereafter disable tqdm equal false thats mean whatever progress bar make true progress bar disabled okay let take first says output directory model name number training epochs thereafter learning rate get per device evaluation batch size thereafter one im gonna put little bit convenient per device train bash size equal batch size per device well size okay somehow error coming let remove error im gonna copy paste size okay batch size also wrong batch size per device check train would eval thats evaluation okay wait dk weight dk weight decay pretraining uh actually retraining evaluation strategy apoc every epoch evaluation done thats mean accuracy matrix calculated right lets go ahead run run says invalid syntax perhaps somewhere might forgot comma okay okay says got something uh okay actually wrong strategy still something missing okay right error coming solution install accelerator upgrade transformers anyway currently using cpu machine time switch machine going switch machine gpu machine second one okay somehow able connect properly im gonna make guess gpu machine gonna connect click ok connecting let zoom little see properly new gpu connected gpu assigned free version google collab total system ram gb disk space gpu space would around gb well right need run everything starting simply come run time say run going run everything starting right first install necessary packages move ahead right one time process going take time im gonna stick code training argument right well wait time let let finish continue okay seems like error getting need run run pip install transformer torch im gonna thereafter need accelerate like suggested seems like need lets go ahead run okay lets go ahead run okay need upgrade accelerate transformers starting im gonna copy im gonna paste starting write pip install upgrade transformers thereafter write pip install upgrade accelerator one installed data set also install upgraded version two libraries anyway need keep upgraded version latest version installed things done restart run together upgrade transformers accelerator data set word umap learn library may take finish lets go ahead wait time finish uh initial training part see training argument already done right needed needed upgrade transformers also accelerate right also discussing compute matrix thats mean evaluation done evaluation done epoch using accuracy f code calculate accuracy evaluation metrics need import sqlarn dot matrix import accuracy score thereafter need f score thereafter write dev thats define compute matrix write print right write labels equal bread label idh thereafter write press equal labels equal print dot label ids prints equal prid weve heard let explain compute matrix takes predicted values predicted values label ids considered thats label thereafter prediction write paid dot predictions dot org max minus thats mean get argument prediction prediction argument thereafter going calculate f score write f equal f score write labels thereafter write average weighted f score f score label spreads average equal weighted thereafter write accuracy write accuracy score write labels reach parades right f equal f score label spreads average weighted got accuracy accuracy score labels prediction one actual labels epoch evaluation done actual levels predicted label passed based accuracy predicted thereafter write return crazy also return f score return f something like compute metrics ready model training write transformers import trainer trainer used training training argument used pass argument training trainer thereafter write trainer equal trainer inside pass model equal model created earlier remember model thats model equal model thereafter going pass arguments arguments training arcs thats training arguments pass compute matrix name compute matrix function made function compute sorry compute matrix right model argument training argument compute compute matrix stored compute matrix let write two guys like thereafter yeah one fact copy paste thereafter write train data set equal emotion encoded remember train data set emotion encoded inside train split thereafter eval data set equal emotions underscore encode validation data set thereafter need also pass tokenizer name tokenizer tokenizer see happens model training argument compute metrics calculated training argument trained data set emotion encoded encoded remember encoded idh thereafter uh thereafter eval data set emotions encoded validation data set encoded token ids attention mask also passing tokenizer lets go ahead run thereafter simply run trainer dot train thats need run training start might need wait let training finish going take time training finished start okay error keep uh got seems like problem compute metrics print dot predictions org max lets go ahead run one one need wait finish training right training done see training loss validation loss loss still lesser training loss see accuracy validation model percent remember earlier starting tutorial told even though imbalanced data set transformers good work imbalance data set apply traditional algorithms like svm sg boost algorithms tfidf may may get may give get better accuracy one transformer really works better trained model got evaluation accuracy time testing model lets go ahead first test accuracy write trades underscore outputs equal trainer dot predict pass emotions encoded test data thereafter write prints underscore outputs dot matrix going prediction test data tell us much accuracy got test data test data got accuracy validation data percent little less test data overall really good accuracy use use model production lets go ahead plot confusion matrix understand particular thing going wrong right wherever getting around percent less accuracy need know classes creating problem write import numpy np thereafter write underscore press equal np dot org max thats going get maximum argument threads output dot predictions say axis equal well true emotions underscore encoded inside test column thereafter take going take roach rows going import level thats says parade output let copy put got vibrate true thereafter need print classification report write sk learn dot matrix import classification report print classes classes thereafter print classification report classification report pass true white bread im sure studying uh transformers im sure know things better like classification reports work confusion matrix clearly see one surprise seems like surprise performance little less right see sadness performance quite good compare compare value counts remember value counts perhaps value counts okay wasnt one anyway somewhere value count let get label counts let uh print label counts understand type labels thats label counts level count see sadness large number tokens large number actually rows accuracy quite good whereas surprise thats see little less accuracy put data model start performing better better right training model already stored data distilled word finetuned emotion name given see distilled word fine tune demotion download data need zip data first zip model thereafter download model get option download right need download one one checkpoints total number runs run today outputs say right jipping use command zip model download use model testing let finally show prediction data set lets say data set text data text data want prediction much simple need write okay im gonna write whole thing make sure see written write input encoded equal kenizer text data return tensors equal pt want device model currently stored thereafter need prediction write torch dot grad thats mean need calculate gradients need write outputs equal model pass inputs thats need output okay seems like tensors device device model also need device let write model equal model dot device right seems like something wrong dont need use thing actually probably let see okay still getting error error shouldnt getting okay okay problem actually two device input encoded need become right thats need run check output get output well logits logits actually predictions write write logits equal outputs dot logits thereafter prediction write trade equal torch dot argument max logits say dimension one see item print spread output get getting one know one simply print classes print would able see exactly name class says joy let change say love love write want die says sadness say want kill probably would anger thats prediction model zip model download load model use model know load model reach commenting video link video ill tell load model right lesson thanks lot watching please subscribe channel share video friends social media thanks lot bye bye take right sample data lets go ahead actual data going fact ill im gonna write tokenizing tokenization emotion data right lets go ahead tokenization emotion data seen emotion data emotional data see remember earlier actually set emotion data emotion data format pandas data format pandas data frame need reset work whole data one go im going reset format earlier emotion dot reset format reset format emotion pandas data frame need tokenization whole data let take explain going seen sample data tokenization done embedding token embedding encoder hidden state implementing birth butt model imported tokenization tokenization generate embedding model generate embedding ill also showing sample embeddings generated butt model thereafter go generate embedding whole data set lets go ahead first tokenization whole data know emotion three split data train validation test want data mean split need implement tokenization method along ill using map function right lets go ahead implement tokenization method write diff tokenize token im gonna pass batch size batch means passing data batches tokenization batches soon get data write tokenize okay something like need write variable maybe template say okay write tokenizer seen earlier batch text data see whatever batch pass text data label well tokenization text column thereafter say padding equal true happens sentences equal length length sentences equal machine learning algorithm need lengths equal case going going padding data length matching padding data zero happens length word length sentence larger case also going truncate truncation happen thereafter im gonna return temp lets go ahead check write print tokenize im gonna pass emotion data seeing going pass first two data check pass first two rows including text label since reading text tokenization text data run right error coming says okay text actually need pass train split okay see clearly first sentence actually small sentence padding done see cls change indicate model need look token thats cls token similarly see attention attention mask attention mask telling us many tokens model need see many tokens model need ignore model need ignore tokens first sentence check one sentence see right two tokenization one tokenization one tokenization maximum length would exactly length first sentence soon pass two maximum length second token maximum length seeing anything one pass even probably would also see okay zero second tokens zeros padded consecutive tokens right thats tokenization tokenization done thereafter need create encoding splits gonna emotions encoded equal emotions dot map pass token h biased equal true write batch size equal none right happened emotions okay seems like emotion emotionmap tokenize means passing method map function says batch true batch size none mean one go pass train split whole data test split whole data validation split whole data thats emotions encoding done done emotions coding lets go ahead see total column names available emotions coding print emotions coding seeing two columns input ids attention mask two additional columns added splits input ids attention mask would able pass whole data generate embedding model right lets go ahead build sample model understand embeddings generated im gonna create first text data write think already text data wrote love machine learning tokenization awesome im gonna create inputs say encoded inputs equal tokenizer pass text data return tensor equal pt one needed return tensor pi torch tensor train model overall would see difference exactly like see okay something wrong okay wait second let see okay say saying got unexpected return tensor second let see think made typo written tensors right getting pi torch tensor although would see difference input output like used see difference created tensor text data love machine learning tokenization awesome lets go ahead build model generate embedding tokens write transformers import auto model right need auto model earlier imported auto tokenizer importing auto model otherwise simply import distill model use simply change name model checkpoint load kind model use auto model write import torch model equal auto model inside write pretrained write model checkpoint model c kptpt thats model checkpoint model print model see download model gonna take time downloaded model lets go ahead check distilbert model layers inside saw earlier first layer first layer generate embeddings thereafter transformer encoder stacks used multihead silver tension technique used generate final output output norm thats output layer output right thats model lets go ahead train model since want train model fact want generate embedding training okay lets go ahead generate embedding model want generate embedding model want model adjust gradients load model purpose write torch dot grad thats mean gradient load gradient make faster model thereafter write outputs equal model write double star write inputs happen input see tensor input ids attention mask thereafter pass two model generated output thereafter write last hidden states equal outputs dot last hidden state see outputs one thats model output model output layers want last hidden state thats last state state want see one see hidden state last hidden state want run see stored last hidden state want inspect simply write outputs would see base model output last hidden state embeddings generated print last hidden states get embedding check shape embedding get total number tokens comma vector length generated distal birth total number tokens present data remember also generating tokens cls separator special tokens right thats generates embedding embedding use classification head model classification data think concept super clear need first tokenize data thereafter need generate embedding need put classification head model prediction two techniques predictions fine tuning one technique attach classification head directly model hidden state retrain model based whatever errors coming classification head thats one technique correct technique produce accuracy another technique got vectors got last hidden states vector simply use kind machine learning algorithm classification vector like use logistic regression svm kind deep learning algorithm cnn rnn whatever algorithm want use vectors problem vectors let tell first benefit benefit fast need train model need retrain model fast problem retrain model sometimes may get correct context use second technique retrain model faster produce less accuracy first method need training produces high accuracy takes little bit time right going pro going follow first technique fine tuning transformer model see fine tuning attaching classification head hidden state also need import auto model sequence classification see digital birth tokenizer imported tensorflow distill tokenization model pi torch distilled model right models cannot use directly classification models trained purposes like masked language predictions see token predictions models trained generally uh token predictions filling mask values something like goal life run see happiness says survival salvations kind mask language change mask lets say happiness delete edge see mask predicted uh mask predicted take load model says h perfectly fitting probability quite high goal life happiness right whenever word missing context use although thats current scope understand use distillery already using steel word emotion classification particular mo particular method going help us load model sequence classification also allow us add additional overhead model lets go ahead import im going write transformers import im going copy thing import auto model sequence classification right thereafter many number labels know classes stored classes simply take length classes thereafter need also find using cuda cpu run torch dot device thereafter gonna use cuda torch dot cuda dot age available cpu check gpu available machine use gpu training model otherwise use cpu lets go ahead create model write model auto model im sorry auto model fact let copy whole thing quite large auto model sequence classification dot pretrained write model c kptpt thats model checkpoint lets go ahead run run tell uh weights digital birth initialized model checkpoint happens weights initialized randomly okay perfectly fine first time load model things missing number labels missing many levels want want total number labels equal number labels thereafter want load model device device thats mean either load model cuda cpu wherever uh q da available load model would check quida available simply print device print device see cpu cpu connected notebook gpu may ask connect since quite large lecture keeping gpu attached google notebook ultimately expire credits unless need train model unless need train model need train model would suggest connect gpu otherwise consume credits time google say consumed credits free credits fact free credits consumed may need wait hours get thats connect gpu although training ill connect gpu right got stuffs ready build model right lets go ahead build model write transformers import training arguments happens transformers import training arguments need build model write bash size equal model name equal already know fact okay model name stored local computer